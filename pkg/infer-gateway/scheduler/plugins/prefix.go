/*
Copyright MatrixInfer-AI Authors.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
*/

package plugins

/*
Prefix Cache Plugin Design

Overview:
The Prefix Cache Plugin is a scoring plugin for the matrixinfer gateway scheduler that implements a prefix-based matching mechanism
for model inference requests. It helps optimize pod scheduling by identifying pods that have previously processed similar
prompts, potentially improving inference performance through cache hits.

Key Components:

1. PrefixCache
   - Main plugin struct implementing the framework.ScorePlugin interface
   - Manages the caching mechanism for model inference requests
   - Configurable parameters for block size and maximum blocks to match

2. ModelPrefixStore
   - Three-level map structure: model -> hash -> pod
   - The LRU cache is used to record hashes. The evicted hash will also be remove from the three-level map above synchronously.
   - During the matching process, only the topK pods with the longest matching prefixes will be returned.

Core Features:

1. Prefix Matching
   - Divides prompts into fixed-size blocks (default: 64 bytes)
   - Generates rolling hashes for each block using xxHash algorithm
   - Each hash is generated by combining the previous block's hash with the current block's content
   - This creates a chain of dependencies where each hash implicitly contains information about all previous blocks
   - When matching hashes, we can start from the end and work backwards:
     * If a hash matches, it guarantees that all previous blocks also match
     * This property allows for efficient prefix matching without checking each block individually
     * The matching length is determined by the position of the first matching hash

2. Scoring Mechanism
   - Scores pods based on the length of matching prefixes
   - Score range: 0-100, calculated as (matching blocks / total blocks) * 100
   - Higher scores indicate better cache hit potential

3. Cache Management
   - LRU-based eviction policy for memory efficiency
   - Automatic cleanup of evicted entries
   - Configurable cache capacity and top-K results

Usage:
The plugin is used in the matrixinfer gateway scheduler framework to score pods based on their potential
for cache hits. It's particularly useful for inference workloads where similar prompts are
likely to be processed multiple times.

Configuration Parameters:
- BlockSizeToHash: Size of each block for hashing (default: 64 bytes)
- MaxBlocksToMatch: Maximum number of blocks to process (default: 128), longer prompts are not processed
- Cache capacity and top-K results are configurable (default: 1000 and 5 respectively)

*/

import (
	"fmt"

	"github.com/cespare/xxhash"

	"matrixinfer.ai/matrixinfer/pkg/infer-gateway/datastore"
	"matrixinfer.ai/matrixinfer/pkg/infer-gateway/scheduler/framework"
	"matrixinfer.ai/matrixinfer/pkg/infer-gateway/scheduler/plugins/cache"
)

const PrefixCachePluginName = "prefix-cache"

const (
	// Default token block size of vLLM is 16, and a good guess of average characters per token is 4.
	// So we use 64 as the default block size.
	BlockSizeToHash = 64
	// TODO: make these parameters configurable.
	MaxBlocksToMatch = 128
	MaxHashCacheSize = 50000
)

var _ framework.ScorePlugin = &PrefixCache{}

type PrefixCache struct {
	name string

	blockSizeToHash  int
	maxBlocksToMatch int
	store            *cache.ModelPrefixStore
}

func NewPrefixCache(store datastore.Store) *PrefixCache {
	p := &PrefixCache{
		name: PrefixCachePluginName,

		blockSizeToHash:  BlockSizeToHash,
		maxBlocksToMatch: MaxBlocksToMatch,
	}
	// Initialize store with default values
	p.store = cache.NewModelPrefixStore(store, MaxHashCacheSize, 5) // TODO: make these configurable
	return p
}

func (p *PrefixCache) Name() string {
	return p.name
}

func (p *PrefixCache) Score(ctx *framework.Context, pods []*datastore.PodInfo) map[*datastore.PodInfo]int {
	scoreResults := make(map[*datastore.PodInfo]int)

	// Initialize all pods with score 0
	for _, pod := range pods {
		scoreResults[pod] = 0
	}

	// Hash the prompt
	hashes := p.hashPrompt(ctx.Model, ctx.Prompt)
	if len(hashes) == 0 {
		return scoreResults
	}

	// Store hashes in context for later use in PostSchedule
	ctx.Hashes = hashes

	// Find pods with matching prefixes
	matches := p.store.FindTopMatches(ctx.Model, hashes, pods)

	// Calculate scores based on prefix match length
	totalHashes := len(hashes)
	for _, match := range matches {
		// Score is the ratio of matching hashes to total hashes, scaled to 0-100
		score := int((float64(match.MatchLen) / float64(totalHashes)) * 100)
		scoreResults[match.Pod] = score
	}

	return scoreResults
}

func (p *PrefixCache) PostSchedule(ctx *framework.Context) {
	if ctx.DecodePod != nil && len(ctx.Hashes) > 0 {
		// Add the selected pod and its hashes to the cache
		p.store.Add(ctx.Model, ctx.Hashes, ctx.DecodePod)
	}

	if ctx.PrefillPod != nil && len(ctx.Hashes) > 0 {
		// Add the selected pod and its hashes to the cache
		p.store.Add(ctx.Model, ctx.Hashes, ctx.PrefillPod)
	}
}

func (p *PrefixCache) hashPrompt(model string, prompt string) []uint64 {
	res := []uint64{}
	if len(prompt) == 0 {
		return res
	}

	// Initialize first block hash
	// Use model name as the first hash to avoid hash collision
	var prevHash uint64 = xxhash.Sum64([]byte(model))
	blockStart := 0

	// Process blocks up to maxBlocksToMatch or until we run out of prompt
	for i := 0; i < p.maxBlocksToMatch && blockStart < len(prompt); i++ {
		// Calculate end position for current block
		blockEnd := blockStart + p.blockSizeToHash
		if blockEnd > len(prompt) {
			blockEnd = len(prompt)
		}

		// Get current block content and combine with previous hash
		block := prompt[blockStart:blockEnd]
		data := []byte(fmt.Sprintf("%d%s", prevHash, block))

		// Use xxHash algorithm
		currHash := xxhash.Sum64(data)

		// Append hash to results
		res = append(res, currHash)

		// Update for next iteration
		prevHash = currHash
		blockStart = blockEnd

		// Break if we've reached the end of prompt
		if blockEnd == len(prompt) {
			break
		}
	}

	return res
}
