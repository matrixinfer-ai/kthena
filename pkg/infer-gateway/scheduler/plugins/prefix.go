package plugins

/*
Prefix Cache Plugin Design

Overview:
The Prefix Cache Plugin is a scoring plugin for the matrixinfer gateway scheduler that implements a prefix-based matching mechanism
for model inference requests. It helps optimize pod scheduling by identifying pods that have previously processed similar
prompts, potentially improving inference performance through cache hits.

Key Components:

1. PrefixCache
   - Main plugin struct implementing the framework.ScorePlugin interface
   - Manages the caching mechanism for model inference requests
   - Configurable parameters for block size and maximum blocks to match

2. ModelPrefixStore
   - Three-level map structure: model -> hash -> pod
   - The LRU cache is used to record hashes. The evicted hash will also be remove from the three-level map above synchronously.
   - During the matching process, only the topK pods with the longest matching prefixes will be returned.

Core Features:

1. Prefix Matching
   - Divides prompts into fixed-size blocks (default: 64 bytes)
   - Generates rolling hashes for each block using xxHash algorithm
   - Each hash is generated by combining the previous block's hash with the current block's content
   - This creates a chain of dependencies where each hash implicitly contains information about all previous blocks
   - When matching hashes, we can start from the end and work backwards:
     * If a hash matches, it guarantees that all previous blocks also match
     * This property allows for efficient prefix matching without checking each block individually
     * The matching length is determined by the position of the first matching hash

2. Scoring Mechanism
   - Scores pods based on the length of matching prefixes
   - Score range: 0-100, calculated as (matching blocks / total blocks) * 100
   - Higher scores indicate better cache hit potential

3. Cache Management
   - LRU-based eviction policy for memory efficiency
   - Automatic cleanup of evicted entries
   - Configurable cache capacity and top-K results

Usage:
The plugin is used in the matrixinfer gateway scheduler framework to score pods based on their potential
for cache hits. It's particularly useful for inference workloads where similar prompts are
likely to be processed multiple times.

Configuration Parameters:
- BlockSizeToHash: Size of each block for hashing (default: 64 bytes)
- MaxBlocksToMatch: Maximum number of blocks to process (default: 128), longer prompts are not processed
- Cache capacity and top-K results are configurable (default: 1000 and 5 respectively)

*/

import (
	"fmt"

	"github.com/cespare/xxhash"
	"k8s.io/apimachinery/pkg/types"

	"matrixinfer.ai/matrixinfer/pkg/infer-gateway/datastore"
	"matrixinfer.ai/matrixinfer/pkg/infer-gateway/scheduler/framework"
)

const PrefixCachePluginName = "prefix-cache"

const (
	BlockSizeToHash  = 64
	MaxBlocksToMatch = 128
)

var _ framework.ScorePlugin = &PrefixCache{}

type PrefixCache struct {
	name string

	blockSizeToHash  int
	maxBlocksToMatch int
	store            *ModelPrefixStore
}

type ModelPrefixStore struct {
	// Three-level map: model -> hash -> pod namespaced name -> pod
	entries map[string]map[uint64]map[types.NamespacedName]*datastore.PodInfo
	lru     Cache[uint64, string]
	topK    int // Each match returns at most topK pods.
}

type matchResult struct {
	pod *datastore.PodInfo
	// The number of matching blocks.
	matchLen int
}

func (p *PrefixCache) newStore(capacity, topK int) *ModelPrefixStore {
	store := &ModelPrefixStore{
		entries: make(map[string]map[uint64]map[types.NamespacedName]*datastore.PodInfo),
		topK:    topK,
	}

	// Create LRU cache with OnEvicted callback to clean up entries
	cache, _ := NewLRUCache[uint64, string](capacity, func(key uint64, value string) {
		hash := key
		model := value

		// Remove pod from the model->hash entry
		if hashMap, exists := store.entries[model]; exists {
			delete(hashMap, hash)
			// If no more hashes for this model, remove the model entry
			if len(hashMap) == 0 {
				delete(store.entries, model)
			}
		}
	})

	store.lru = cache
	return store
}

// findTopMatches finds the topK pods with the longest matching prefixes for given model and hashes
func (s *ModelPrefixStore) findTopMatches(model string, hashes []uint64, pods []*datastore.PodInfo) []matchResult {
	matches := make([]matchResult, 0, s.topK)

	// Only check entries for the requested model
	modelEntries, exists := s.entries[model]
	if !exists {
		return matches
	}

	// Track processed pods to avoid duplicates
	processedPods := make(map[*datastore.PodInfo]bool)

	// Start matching from the end of hashes
	// This works because each hash depends on the previous hash in hashPrompt
	for i := len(hashes) - 1; i >= 0; i-- {
		hash := hashes[i]
		// Check if this hash exists in cache
		if !s.lru.Contains(hash) {
			continue
		}
		if podMap, exists := modelEntries[hash]; exists {
			for _, pod := range podMap {
				// Skip if pod is not in the candidate set or already processed
				if processedPods[pod] {
					continue
				}
				processedPods[pod] = true

				// If we found a match at position i, we know all previous hashes must match
				// because each hash depends on the previous one in hashPrompt
				matchLen := i + 1

				matches = append(matches, matchResult{
					pod:      pod,
					matchLen: matchLen,
				})

				// Return if we have enough matches
				if len(matches) >= s.topK {
					return matches
				}
			}
		}
	}

	return matches
}

// add adds new hash->pod mappings to cache, using LRU for eviction
func (s *ModelPrefixStore) add(model string, hashes []uint64, pod *datastore.PodInfo) {
	nsName := types.NamespacedName{
		Namespace: pod.Pod.Namespace,
		Name:      pod.Pod.Name,
	}

	// Initialize model map if it doesn't exist
	if _, exists := s.entries[model]; !exists {
		s.entries[model] = make(map[uint64]map[types.NamespacedName]*datastore.PodInfo)
	}

	// Add pod to each hash's pod map from start to end
	// This ensures that when LRU eviction happens, we keep the most important (last) hashes
	for i := 0; i < len(hashes); i++ {
		hash := hashes[i]
		if _, exists := s.entries[model][hash]; !exists {
			s.entries[model][hash] = make(map[types.NamespacedName]*datastore.PodInfo)
		}
		s.entries[model][hash][nsName] = pod

		// Add to LRU with hash as key and model as value
		s.lru.Add(hash, model)
	}
}

// Helper function to check if a pod exists in a map
func hasPod(podMap map[types.NamespacedName]*datastore.PodInfo, pod *datastore.PodInfo) bool {
	nsName := types.NamespacedName{
		Namespace: pod.Pod.Namespace,
		Name:      pod.Pod.Name,
	}
	_, exists := podMap[nsName]
	return exists
}

func NewPrefixCache() *PrefixCache {
	p := &PrefixCache{
		name: PrefixCachePluginName,

		// TODO: make these parameters configurable.
		blockSizeToHash:  BlockSizeToHash,
		maxBlocksToMatch: MaxBlocksToMatch,
	}
	// Initialize store with default values
	p.store = p.newStore(1000, 5) // TODO: make these configurable
	return p
}

func (p *PrefixCache) Name() string {
	return p.name
}

func (p *PrefixCache) Score(pods []*datastore.PodInfo, ctx *framework.Context) map[*datastore.PodInfo]int {
	scoreResults := make(map[*datastore.PodInfo]int)

	// Initialize all pods with score 0
	for _, pod := range pods {
		scoreResults[pod] = 0
	}

	// Hash the prompt
	hashes := p.hashPrompt(ctx.Model, ctx.Prompt)
	if len(hashes) == 0 {
		return scoreResults
	}

	// Store hashes in context for later use in PostSchedule
	ctx.Hashes = hashes

	// Find pods with matching prefixes
	matches := p.store.findTopMatches(ctx.Model, hashes, pods)

	// Calculate scores based on prefix match length
	totalHashes := len(hashes)
	for _, match := range matches {
		// Score is the ratio of matching hashes to total hashes, scaled to 0-100
		score := int((float64(match.matchLen) / float64(totalHashes)) * 100)
		scoreResults[match.pod] = score
	}

	return scoreResults
}

func (p *PrefixCache) PostSchedule(ctx *framework.Context) {
	if ctx.TargetPod != nil && len(ctx.Hashes) > 0 {
		// Add the selected pod and its hashes to the cache
		p.store.add(ctx.Model, ctx.Hashes, ctx.TargetPod)
	}
}

func (p *PrefixCache) hashPrompt(model string, prompt string) []uint64 {
	res := []uint64{}
	if len(prompt) == 0 {
		return res
	}

	// Initialize first block hash
	var prevHash uint64 = 0
	blockStart := 0

	// Process blocks up to maxBlocksToMatch or until we run out of prompt
	for i := 0; i < p.maxBlocksToMatch && blockStart < len(prompt); i++ {
		// Calculate end position for current block
		blockEnd := blockStart + p.blockSizeToHash
		if blockEnd > len(prompt) {
			blockEnd = len(prompt)
		}

		// Get current block content and combine with previous hash
		block := prompt[blockStart:blockEnd]
		data := []byte(fmt.Sprintf("%d%s", prevHash, block))

		// Use xxHash algorithm
		currHash := xxhash.Sum64(data)

		// Append hash to results
		res = append(res, currHash)

		// Update for next iteration
		prevHash = currHash
		blockStart = blockEnd

		// Break if we've reached the end of prompt
		if blockEnd == len(prompt) {
			break
		}
	}

	return res
}
