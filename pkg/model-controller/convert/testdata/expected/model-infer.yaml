apiVersion: workload.matrixinfer.ai/v1alpha1
kind: modelinfer
metadata:
  labels:
    model.uid: randomUID
  name: test-model-backend1
  namespace: default
  ownerReferences:
    - apiVersion: registry.matrixinfer.ai/v1alpha1
      kind: Model
      name: test-model
      uid: randomUID
spec:
  recoveryPolicy: InferGroupRestart
  replicas: 0
  rolloutStrategy:
    type: InferGroupRollingUpdate
  schedulerName: ""
  template:
    gangSchedule: {}
    networkTopology:
      highestTierAllowed: 2
      mode: hard
    restartGracePeriodSeconds: 60
    roles:
      - entryTemplate:
          metadata:
            labels:
              model.uid: randomUID
          spec:
            containers:
              - args:
                  - --port
                  - "8900"
                  - --engine
                  - vllm
                  - --engine-base-url
                  - http://localhost:8000
                  - --engine-metrics-path
                  - /metrics
                env:
                  - name: ENDPOINT
                    value: https://obs.test.com
                  - name: RUNTIME_PORT
                    value: "8900"
                envFrom:
                  - secretRef:
                      name: "test-secret"
                image: matrixinfer/runtime:latest
                name: runtime
                ports:
                  - containerPort: 8900
                readinessProbe:
                  httpGet:
                    path: /health
                    port: 8900
                  initialDelaySeconds: 5
                  periodSeconds: 10
                resources: {}
              - command:
                  - bash
                  - -c
                  - chmod u+x /vllm-workspace/vllm/examples/online_serving/multi-node-serving.sh
                    && /vllm-workspace/vllm/examples/online_serving/multi-node-serving.sh
                    leader --ray_cluster_size=2 --num-gpus=1 && python -m vllm.entrypoints.openai.api_server
                    --model /tmp/test/13a7c4d58031cdc502ca1bb4a592f2b9 --block-size 128
                    --gpu-memory-utilization 0.9 --max-model-len 32768 --served-model-name deepseek-v3
                    --tensor-parallel-size 2 --trust-remote-code --distributed_executor_backend ray
                  - --enable-lora
                  - --lora-modules
                  - lora-sql=/tmp/test/11e9a15be1e19ff056520bf67eaeb688
                image: vllm-server:latest
                lifecycle:
                  preStop:
                    exec:
                      command:
                        - /bin/sh
                        - -c
                        - |
                          while true; do
                            RUNNING=$(curl -s http://localhost:8000/metrics | grep 'vllm:num_requests_running' | grep -v '#' | awk '{print $2}')
                            WAITING=$(curl -s http://localhost:8000/metrics | grep 'vllm:num_requests_waiting' | grep -v '#' | awk '{print $2}')
                            if [ "$RUNNING" = "0.0" ] && [ "$WAITING" = "0.0" ]; then
                              echo "Terminating: No active or waiting requests, safe to terminate" >> /proc/1/fd/1
                              exit 0
                            else
                              echo "Terminating: Running: $RUNNING, Waiting: $WAITING" >> /proc/1/fd/1
                              sleep 5
                            fi
                          done
                name: engine
                env:
                  - name: "ENDPOINT"
                    value: "https://obs.test.com"
                  - name: RUNTIME_PORT
                    value: "8900"
                readinessProbe:
                  failureThreshold: 3
                  httpGet:
                    path: /health
                    port: 8000
                    scheme: HTTP
                  initialDelaySeconds: 90
                  periodSeconds: 5
                  successThreshold: 1
                  timeoutSeconds: 1
                resources:
                  requests:
                    cpu: 100m
                    huawei.com/ascend-1980: "1"
                    memory: 1Gi
                volumeMounts:
                  - mountPath: /tmp/test
                    name: backend1-weights
            initContainers:
              - args:
                  - --source
                  - s3://aios_models/deepseek-ai/DeepSeek-V3-W8A8/vllm-ascend
                  - --output-dir
                  - /tmp/test/13a7c4d58031cdc502ca1bb4a592f2b9
                image: matrixinfer/downloader:latest
                env:
                  - name: "ENDPOINT"
                    value: "https://obs.test.com"
                envfrom:
                  - secretRef:
                      name: "test-secret"
                name: test-model-model-downloader
                resources: {}
                volumeMounts:
                  - mountPath: /tmp/test
                    name: backend1-weights
              - args:
                  - --source
                  - s3://aios_models/deepseek-ai/DeepSeek-V3-W8A8/vllm-ascend-lora
                  - --output-dir
                  - /tmp/test/11e9a15be1e19ff056520bf67eaeb688
                env:
                  - name: ENDPOINT
                    value: https://obs.test.com
                  - name: RUNTIME_PORT
                    value: "8900"
                envFrom:
                  - secretRef:
                      name: "test-secret"
                image: matrixinfer/downloader:latest
                name: test-model-lora-downloader-0
                resources: { }
                volumeMounts:
                  - mountPath: /tmp/test
                    name: backend1-weights
            terminationGracePeriodSeconds: 300
            volumes:
              - hostPath:
                  path: /tmp/test
                  type: DirectoryOrCreate
                name: backend1-weights
        name: leader
        networkTopology:
          highestTierAllowed: 2
          mode: hard
        replicas: 0
        workerReplicas: 1
        workerTemplate:
          spec:
            containers:
              - command:
                  - bash
                  - -c
                  - chmod u+x /vllm-workspace/vllm/examples/online_serving/multi-node-serving.sh
                    && /vllm-workspace/vllm/examples/online_serving/multi-node-serving.sh
                    worker --ray_address=$(ENTRY_ADDRESS)
                env:
                  - name: "ENDPOINT"
                    value: "https://obs.test.com"
                  - name: RUNTIME_PORT
                    value: "8900"
                image: vllm-server:latest
                name: backend1-vllm-worker
                resources:
                  requests:
                    cpu: 100m
                    huawei.com/ascend-1980: "1"
                    memory: 1Gi
                volumeMounts:
                  - mountPath: /tmp/test
                    name: backend1-weights
            initContainers:
              - args:
                  - --source
                  - s3://aios_models/deepseek-ai/DeepSeek-V3-W8A8/vllm-ascend
                  - --output-dir
                  - /tmp/test/13a7c4d58031cdc502ca1bb4a592f2b9
                image: matrixinfer/downloader:latest
                env:
                  - name: "ENDPOINT"
                    value: "https://obs.test.com"
                envfrom:
                  - secretRef:
                      name: "test-secret"
                name: test-model-model-downloader
                resources: { }
                volumeMounts:
                  - mountPath: /tmp/test
                    name: backend1-weights
              - args:
                  - --source
                  - s3://aios_models/deepseek-ai/DeepSeek-V3-W8A8/vllm-ascend-lora
                  - --output-dir
                  - /tmp/test/11e9a15be1e19ff056520bf67eaeb688
                env:
                  - name: ENDPOINT
                    value: https://obs.test.com
                  - name: RUNTIME_PORT
                    value: "8900"
                envFrom:
                  - secretRef:
                      name: "test-secret"
                image: matrixinfer/downloader:latest
                name: test-model-lora-downloader-0
                resources: { }
                volumeMounts:
                  - mountPath: /tmp/test
                    name: backend1-weights
            volumes:
              - hostPath:
                  path: /tmp/test
                  type: DirectoryOrCreate
                name: backend1-weights
  topologySpreadConstraints:
    - labelSelector:
        matchLabels:
          modelinfer/name: test-model
      maxSkew: 1
      topologyKey: kubernetes.io/hostname
      whenUnsatisfiable: DoNotSchedule