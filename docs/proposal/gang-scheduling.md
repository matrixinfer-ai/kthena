---
title: Gang Scheduling for ModelInfer Workloads
authors:
- "@VanderChen"
reviewers:
- TBD
approvers:
- TBD

creation-date: 2025-08-06
status: draft

---

## Gang Scheduling for ModelInfer Workloads

<!--
This is the title of your KEP. Keep it short, simple, and descriptive. A good
title can help communicate what the KEP is and should be considered as part of
any review.
-->

### Summary

<!--
This section is incredibly important for producing high-quality, user-focused
documentation such as release notes or a development roadmap.

A good summary is probably at least a paragraph in length.
-->

This proposal describes a design for implementing `gang` scheduling for `ModelInfer` using Volcano's PodGroup:

1. The entire `ModelInfer` instance (InferGroups) level, ensuring that all pods in an `InferGroup` are scheduled together.
2. Allowing the number of replicas needed to be specified for a Role in an `InferGroup`, enabling xPyD deployment.
3. Support networkTopology of PodGroup generated by `InferGroup`.

### Motivation

<!--
This section is for explicitly listing the motivation, goals, and non-goals of
this KEP.  Describe why the change is important and the benefits to users.
-->

#### Goals

<!--
List the specific goals of the KEP. What is it trying to achieve? How will we
know that this has succeeded?
-->

**gang scheduling**: Implement gang scheduling at [InferGroup level](https://github.com/matrixinfer-ai/matrixinfer/blob/main/pkg/apis/workload/v1alpha1/infergroup_types.go)
**InferGroup Level NetworkTopology**: Implement InferGroup level [network topology](https://volcano.sh/en/docs/network_topology_aware_scheduling/)

#### Non-Goals

<!--
What is out of scope for this KEP? Listing non-goals helps to focus discussion
and make progress.
-->

1. **Custom scheduler**: We will not implement a custom scheduler, but use Volcano's existing capabilities
2. **Complex scheduling policies**: Focus on basic gang scheduling, not advanced scheduling policies
3. **Multi-cluster scheduling**: This design is focused on single-cluster scheduling

### Proposal

<!--
This is where we get down to the specifics of what the proposal actually is.
This should have enough detail that reviewers can understand exactly what
you're proposing, but should not include things like API designs or
implementation. What is the desired outcome and how do we measure success?.
The "Design Details" section below is for the real
nitty-gritty.
-->

#### User Stories (Optional)

<!--
Detail the things that people will be able to do if this KEP is implemented.
Include as much detail as possible so that people can understand the "how" of
the system. The goal here is to make this feel real for users without getting
bogged down.
-->

##### Story 1: ModelInfer instance gang scheduling

InferGroups are the top-level scheduling units in ModelInfer. Each InferGroup represents a single ModelInfer instance.

###### Scenario 1: Fix ratio prefill decode disaggregation

In this scenario, we want to ensure that all pods of a infergroup are scheduled together.
For example, a ModelInfer instance has 2 roles: `prefiller` and `decoder`.
The `prefiller` role has 1 pod, and the `decoder` role has 3 pods.

```yaml
kind: ModelInfer
metadata:
  name: sample
spec:
  replicas: 1  # inferGroup replicas
  template:
    roles:
      - name: prefill
        replicas: 1
        ...       # role replicas, for example, 1P3D
      - name: decode
        replicas: 3  # role replicas, for example, 1P3D
        ...
```

we want to ensure the `prefiller` pod and the 3 `decoder` pods are scheduled together.
When scale up, another `prefiller` pod and 3 `decoder` pods will be scheduled together.

```yaml
kind: ModelInfer
metadata:
  name: sample
spec:
  replicas: 2  # inferGroup replicas
  template:
    roles:
      - name: prefill
        replicas: 1
        ...       # role replicas, for example, 1P3D
      - name: decode
        replicas: 3  # role replicas, for example, 1P3D
        ...
```

#### Notes/Constraints/Caveats (Optional)

<!--
What are the caveats to the proposal?
What are some important details that didn't come across above?
Go in to as much detail as necessary here.
This might be a good place to talk about core concepts and how they relate.
-->

#### Risks and Mitigations

<!--
What are the risks of this proposal, and how do we mitigate?

How will security be reviewed, and by whom?

How will UX be reviewed, and by whom?

Consider including folks who also work outside the SIG or subproject.
-->

### Design Details

<!--
This section should contain enough information that the specifics of your
change are understandable. This may include API specs (though not always
required) or even code snippets. If there's any ambiguity about HOW your
proposal will be implemented, this is the place to discuss them.
-->

#### API Changes

The design uses an enum-based approach for gang scheduling instead of separate boolean fields, providing better clarity and preventing invalid combinations:

It only works when `GangSchedule` is configured.

```go
// GangSchedule defines the gang scheduling configuration.
type GangSchedule struct {
  // NetworkTopology defines the NetworkTopology config, this field works in conjunction with network topology feature and hyperNode CRD.
  // Fields can only be configured if enable=true
  // +optional
  NetworkTopology *volcanoV1Beta1.NetworkTopologySpec `json:"networkTopology,omitempty" protobuf:"bytes,5,opt,name=networkTopology"`

  // MinRoleReplicas defines the minimum number of replicas required for each role
  // This map allows users to specify different
  // minimum replica requirements for different roles.
  // Key: role name, Value: minimum number of replicas required for that role
  // +optional
  MinRoleReplicas map[string]int32 `json:"minRoleReplicas,omitempty"`
}
```

Use [`MinTaskMember`](https://github.com/volcano-sh/volcano/blob/master/docs/design/task-minavailable.md) to control the gang-scheduling scope.

MinTaskMember is a map where:

- **Key**: `{role name + role replica index}` (e.g., "prefill-0", "decode-1")
- **Value**: Number of pods required for that specific role replica

This allows fine-grained control over gang scheduling at both instance and role levels by defining the minimum number of pods required for each role replica within a PodGroup.

Volcano PodGroup can set the topology constraints of the job through the networkTopology field. Specific documentation on the NetworkTopology can be refer to [volcano doc](https://volcano.sh/en/docs/network_topology_aware_scheduling/).

**Note**: If you want to use a network topology, you need to use [Volcano's HyperNode](https://github.com/volcano-sh/apis/blob/network-topology-dev/pkg/apis/topology/v1alpha1/hypernode_types.go) to represent the network topology.

Supporting the following configurations:

- **mode**: Supports hard and soft modes
  - hard: Hard constraint, tasks within the job must be deployed within the same HyperNode.
  - soft: Soft constraint, tasks are deployed within the same HyperNode as much as possible.
- **highestTierAllowed**: Used with hard mode, indicating the highest tier of HyperNode allowed for job deployment. This field is not required when mode is soft.

For example, the following configuration means the job can only be deployed within HyperNodes of tier 2 or lower, such as s4 and s5, and their child nodes s0, s1, s2, s3. Otherwise, the job will remain in the Pending state:

```yaml
spec:
  networkTopology:
    mode: hard
    highestTierAllowed: 2
```

Through this scheduling strategy, users can precisely control the network topology constraints of the job, ensuring that the job runs in the best performance domain that meets the conditions, thereby significantly improving training efficiency.

#### Controller Architecture

The gang scheduling implementation is integrated into the existing ModelInfer controller architecture with the following components:

##### Gang Scheduling Manager

A dedicated gang scheduling manager within the ModelInfer controller handles PodGroup lifecycle management. The manager is responsible for:

- Creating and managing PodGroups based on the configured gang scheduling
- Cleaning up PodGroups when gang scheduling is disabled
- Handling PodGroup updates during scaling operations

##### PodGroup Creation Strategy

**Group-Level Gang Scheduling:**

- Creates one PodGroup per ModelInfer replica (each InferGroup)
- PodGroup name: `{modelinfer-name}-{infergroup-index}`
- Each role instance becomes a task in the PodGroup
- MinTaskMember includes all role instances within the InferGroup

**Important Note:** PodGroups are created for each ModelInfer replica (each InferGroup). If `MinRoleReplicas` is configured, `minTaskMember` in `podGroups` will be configured based on `MinRoleReplicas`. if not configured, minTaskMember will be configured based on the role's replicas.

##### Pod Annotation Strategy

All pods created by the ModelInfer controller are annotated with the appropriate PodGroup information using the `scheduling.k8s.io/group-name` annotation. The annotation value corresponds to the PodGroup name that the pod should belong to, enabling Volcano scheduler to enforce gang scheduling constraints.

In addition to this, it is necessary to annotation is `volcano.sh/task-spec`. Used to indicate which volcano podGroup task the pod belongs to.

#### Scaling and Lifecycle Management

##### Scale-Up Operations

When scaling up ModelInfer replicas, the controller creates new PodGroups for the newly added InferGroups.

##### Scale-Down Operations

When scaling down, the controller removes PodGroups associated with the scaled-down InferGroups:

#### Instance-Level Gang Scheduling

##### Creation Process

A single PodGroup is created for the entire ModelInfer instance.

**PodGroup Naming Convention:**

- Name: `{modelinfer-name}-instance`
- Labels and annotations include ModelInfer metadata

**PodGroup Configuration:**

```yaml
apiVersion: scheduling.volcano.sh/v1beta1
kind: PodGroup
metadata:
  name: {modelinfer-name}-{inferGroup-index}
  namespace: {modelinfer-namespace}
  labels:
    modelinfer.matrixinfer.ai/name: {modelinfer-name}-{inferGroup-index}
  annotations:
    scheduling.k8s.io/group-name: {modelinfer-name}
spec:
  # MinTaskMember defines minimum pods required for each role replica
  minTaskMember:
    "prefill-0": 4    # 1 entry + 3 workers for prefill role replica 0
    "decode-0": 4     # 1 entry + 3 workers for decode role replica 0
    # ... additional role replicas
  minResources:
    # Aggregated resource requirements
  ttlSecondsAfterFinished: {timeout-seconds}
```

**Pod Count Calculation:**
If `MinRoleReplicas` is not configured, the value of `minTaskMember` is calculated as:

```math
minMember = replicas × Σ(role.replicas × (1 + role.workerReplicas))
```

Where:

- `replicas`: Number of InferGroup instances
- `role.replicas`: Number of role instances within each InferGroup
- `1 + role.workerReplicas`: Entry pod + worker pods per role instance

If `MinRoleReplicas` is configured, the value of `minMember` is calculated as:

**MinTaskMember Generation Logic:**

- For each role specified in MinRoleReplicas map
- Include role replicas from index 0 up to MinRoleReplicas[roleName] - 1
- Each role replica entry has value = (1 + role.workerReplicas)

**Example Configuration:**

```yaml
gangSchedule:
  minRoleReplicas:
    prefill: 2    # Require at least 2 prefill role replicas
    decode: 1     # Require at least 1 decode role replica
```

#### PodGroup Examples

For detailed YAML examples of PodGroups created from ModelInfer instances, see [podgroup-examples.yaml](./podgroup-examples.yaml).

The examples demonstrate:

1. **Instance-level gang scheduling**: Single PodGroup for entire ModelInfer
2. **Role-level gang scheduling**: Multiple PodGroups, one per role per InferGroup
3. **Gang scheduling disabled**: No PodGroups created

#### Test Plan

<!--
**Note:** *Not required until targeted at a release.*

Consider the following in developing a test plan for this enhancement:
- Will there be e2e and integration tests, in addition to unit tests?
- How will it be tested in isolation vs with other components?

No need to outline all test cases, just the general strategy. Anything
that would count as tricky in the implementation, and anything particularly
challenging to test, should be called out.

-->

### Alternatives

<!--
What other approaches did you consider, and why did you rule them out? These do
not need to be as detailed as the proposal, but should include enough
information to express the idea and why it was not acceptable.
-->

<!--
Note: This is a simplified version of kubernetes enhancement proposal template.
https://github.com/kubernetes/enhancements/tree/3317d4cb548c396a430d1c1ac6625226018adf6a/keps/NNNN-kep-template
-->
