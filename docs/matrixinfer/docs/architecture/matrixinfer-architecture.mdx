import LightboxImage from '@site/src/components/LightboxImage';

# Architecture Overview

MatrixInfer is a Kubernetes-native AI inference platform built on a **two-plane architecture** designed for scalability, observability, and efficiency. The system separates **control plane operations** and **data plane execution** into distinct architectural planes.

## High-Level Architecture

The platform comprises two primary planes:

- **Control Plane**: Manages models, routes, servers, inference configurations, and autoscaling policies through Kubernetes Custom Resource Definitions (CRDs) and Controllers. Controllers continuously reconcile these CRDs into runtime resources.
- **Data Plane**: Executes inference workloads through the Gateway and Scheduler, which orchestrate request routing and scheduling. Inference Pods execute AI model requests using **role-based replica groups** supporting Prefill/Decode disaggregation.

<LightboxImage src="/img/diagrams/architecture/architecture_overview.svg" alt="Architecture Overview"/>

## Core Components

### 1. **Custom Resource Definitions (CRDs)**

MatrixInfer extends Kubernetes with custom resources that provide declarative configuration for AI inference workloads:

- **Model** – A high-level, opinionated API that captures model specifications and cascades creation/update/deletion of related resources (ModelRoute, ModelServer, ModelInfer, autoscaling artifacts) to deliver a one-stop model deployment experience.
- **ModelRoute** – Declares routing intent and traffic control: matches by `modelName`, `loraAdapters`, and HTTP attributes (path, headers); supports rule-based routing with weights, canary/A–B strategies, and optional token-based `rateLimit`s.
- **ModelServer** – Defines service exposure and traffic policy for backend inference pods: specifies the model and inference framework/engine, discovers pods via `workloadSelector` (including role-aware groupings such as Prefill/Decode), and applies traffic policies (retries, timeouts, connection settings) for healthy endpoint publication.
- **ModelInfer** – Manages inference workloads as `InferGroup`s with role-based replicas (e.g., Prefill/Decode), supporting configurable entry/worker Pod `template`s, `recoveryPolicy`, topology- and gang-aware scheduling, rolling upgrades, and a dedicated `scheduler`.
- **AutoScalingPolicy** – Specifies autoscaling rules: metric endpoints and triggers (CPU, memory, custom), target thresholds, and scaling behaviors (up/down rates, stabilization and panic windows) for automated replica management.
- **AutoScalingPolicyBinding** – Attaches policies to target workloads (e.g., ModelInfer or Deployments), supporting two modes: **scalingConfiguration** for metric-driven scaling of a single target and **optimizerConfiguration** for cost-aware distribution across heterogeneous instance types and engines (e.g., H100/A100).

Platform operators manage these CRDs declaratively, and Control Plane controllers continuously reconcile them into runtime resources.

### 2. **Control Plane**

The Control Plane ensures that declarative configurations are realized into operational resources through continuous reconciliation of CRDs into runtime resources.

#### **Controllers**

- **Model Controller** – Reconciles `Model` resources into downstream primitives (`ModelRoute`, `ModelServer`, `ModelInfer`, `AutoScalingPolicy`, `AutoScalingPolicyBinding`) and maintains overall model lifecycle and status. It propagates updates and orchestrates cascaded create/update/delete to keep derived resources consistent.
- **ModelRoute Controller** – Projects routing intent into the Infer Gateway by continuously syncing matches, weights, and model/LoRA selectors from `ModelRoute` CRs. It validates and normalizes rule sets to produce the effective routing table used by the gateway.
- **ModelServer Controller** – Tracks backend pods via `workloadSelector`, applies traffic policies (retries, timeouts, connection settings), and publishes healthy endpoints to the gateway. It manages service exposure and connectivity for model-serving backends.
- **ModelInfer Controller** – Manages `ModelInfer` workloads including `InferGroup`s and role-based replicas (e.g., Prefill/Decode). It handles topology- and gang-aware placement, fault recovery, and rolling upgrades, and reconciles entry/worker Pod templates and services for each role.
- **Autoscaler Controller** – Evaluates runtime metrics against `AutoScalingPolicy` targets and computes desired replica counts for bound workloads. It supports homogeneous scaling (stable/panic modes) and heterogeneous optimization via `AutoScalingPolicyBinding`, adjusting replicas and instance mix to meet SLOs and cost goals.

### 3. **Data Plane**

The Data Plane executes inference workloads and handles request processing through the Gateway and Scheduler, using optimized, role-based pod architectures that support both homogeneous and heterogeneous scaling strategies.

#### **Infer Gateway**

The Infer Gateway processes user requests through a comprehensive pipeline that ensures security, fairness, and optimal resource utilization:

##### **Request Pipeline**

The request pipeline orchestrates the flow of user requests through the following stages:

*   **Authentication & Authorization** – Validates user identity and permissions
*   **Rate Limiting** – Enforces request throughput limits to prevent system overload
*   **Fairness Scheduling** – Implements queuing mechanisms and fair resource allocation
*   **Scheduling** – Selects optimal pods using filter and score plugins for intelligent request routing.
    <details>
        <summary><b>Scheduler (Gateway Component Details)</b></summary>
        <p>The Infer Gateway includes a pluggable Scheduler that plays a crucial role in the "Scheduling" stage of the request pipeline. It employs <b>advanced scheduling plugins</b> to optimize request routing and resource utilization:</p>
        ###### Filter Plugins:
        <ul>
            <li><i>Least Requests</i> – Filters pods based on current request load</li>
            <li><i>LoRA Affinity</i> – Ensures requests requiring specific LoRA adapters are routed to compatible pods</li>
        </ul>
        ###### Score Plugins:
        <ul>
            <li><i>KV Cache Aware</i> – Optimizes routing based on key-value cache availability and utilization</li>
            <li><i>Least Latency</i> – Minimizes Time to First Token (TTFT) and Time Per Output Token (TPOT)</li>
            <li><i>Prefix Cache</i> – Leverages shared prefix caching for improved performance</li>
            <li><i>GPU Cache</i> – Considers GPU memory cache status for optimal routing</li>
        </ul>
        <p>It seamlessly integrates with the Infer Gateway’s Load Balancing and Fairness Scheduling components to ensure optimal request distribution.</p>
    </details>

*   **Load Balancing** – Routes requests to optimal backend instances based on health and capacity
*   **Proxy** – Dispatches requests to appropriate data plane inference groups


#### **Inference Pods**

Inference workloads are organized into **Groups** containing **multiple Replicas**. Each replica can assume specialized **roles** to optimize different phases of the inference process:

**Role-Based Architecture:**
- **Prefill Role** – Handles prompt initialization and context processing
- **Decode Role** – Manages incremental token generation and output streaming

**Pod Components:**
Each replica deployment may include the following components:

- **Entry Pod** – Provides ingress endpoints for role-specific requests
- **Worker Pod(s)** – Execute actual model inference computations
- **Init Container** – Handles dependency resolution and artifact setup prior to execution
- **Sidecar Container** – Manages logging, observability, and networking auxiliary processes
- **Downloader** – Fetches model weights and artifacts from storage
- **Runtime Agent** – Collects health metrics and performance telemetry
- **LLM Engines** – Integrates with specialized backends (e.g., **vLLM**, **SGLang**)

This architecture enables **Prefill/Decode Disaggregation (PD mode)**, allowing independent scaling of different inference stages for optimal resource utilization and performance.

## Request Processing Pipeline

<div style={{ textAlign: 'center' }}>
    <LightboxImage src="/img/diagrams/architecture/request_pipeline.svg" alt="Request Processing Pipeline" />
</div>

1.  **Request Ingress (Client Request Initiation):**
    *   Client applications submit inference requests to the **Infer Gateway** via REST/gRPC endpoints.
2.  **Gateway Request Processing (Authentication & Rate Control):**
    *   The **Infer Gateway** validates client credentials and enforces **Rate Limiting** policies to prevent system overload.
3.  **Fairness Scheduling & Queue Management:**
    *   The **Fairness Scheduler** implements queue-based resource allocation to ensure equitable request distribution and prevent resource starvation when capacity is exceeded.
4.  **Intelligent Request Routing (Scheduler Optimization):**
    *   The **Scheduler** evaluates available **Inference Pods** using a two-stage selection process:
    *   **Filter Plugins** eliminate unsuitable pods based on resource constraints and compatibility (e.g., LoRA requirements).
    *   **Score Plugins** rank pods using criteria including request load (**Least Requests**), cache utilization (**KV Cache Aware**, **GPU Cache**), latency optimization (**Least Latency**), and prefix caching (**Prefix Cache**).
5.  **Load Balancing & Request Dispatching:**
    *   The **Load Balancer** routes requests to optimal backend instances based on health checks and capacity metrics derived from Control Plane metadata.
    *   The **Proxy** component handles request forwarding and maintains connection pooling to target inference groups.
6.  **Inference Pod Execution:**
    *   **Inference Pods** execute model computations within specialized role-based architectures:
    *   **Prefill Pods:** Process input tokenization, context encoding, and KV cache initialization for prompt understanding.
    *   **Decode Pods:** Perform autoregressive token generation and output streaming for response completion.
    *   Each pod integrates **LLM Engines** (vLLM, SGLang) for optimized inference execution.
    *   Supporting components include **Init Containers** for model artifact retrieval, **Sidecar Containers** with **Runtime Agents** for telemetry collection and health monitoring.
7.  **Control Plane Orchestration (Resource Lifecycle Management):**
    *   **Platform Operators** manage model deployments through CRD manipulation of **Model** resources.
    *   The **Model Controller** propagates configuration changes to downstream controllers: **ModelRoute Controller** (routing rule synchronization), **ModelServer Controller** (endpoint configuration), and **ModelInfer Controller** (replica group orchestration).
    *   **AutoScaling Policies** define scaling rules with metric endpoints, thresholds, and behaviors, bound to workloads via **AutoScalingPolicyBinding** resources supporting both standard scaling and cost-optimized distribution across heterogeneous hardware.
    *   The **Autoscaler Controller** monitors target pod metrics, evaluates scaling policies, and instructs the **ModelInfer Controller** to adjust replica counts or redistribute workloads across instance types for optimal resource utilization and cost efficiency.

## Key Features

MatrixInfer provides comprehensive capabilities for enterprise-grade AI inference deployment:

- **Declarative Management**: Complete CRD-driven configuration for models, routes, servers, and scaling policies
- **Control Plane Orchestration**: Automated resource management with dedicated controllers and continuous reconciliation
- **Advanced Gateway Pipeline**: Comprehensive request processing with authentication, rate limiting, fairness scheduling, and intelligent routing
- **Intelligent Scheduling**: Pluggable framework with latency-aware, cache-aware, and LoRA affinity optimizations
- **Prefill/Decode Disaggregation**: Independent scaling of inference stages through specialized workload separation
- **Role-Based Architecture**: Flexible inference groups with granular replica management and role assignments
- **Enterprise Observability**: Comprehensive monitoring via Init Containers, Sidecar Containers, and Runtime Agents
- **Dynamic Scaling**: Metric-driven autoscaling supporting both homogeneous and heterogeneous instance types
- **Multi-Engine Support**: Native integration with leading LLM inference engines (vLLM, SGLang)
- **Kubernetes-Native**: Full ecosystem integration including RBAC, networking, and storage