# llm-d inference simulator for Llama-3.1-8B-Instruct model
# This example demonstrates how to route inference requests to a simulated LLM inference server using vLLM.
apiVersion: apps/v1
kind: Deployment
metadata:
  name: vllm-llama3-8b-instruct
spec:
  replicas: 3
  selector:
    matchLabels:
      app: vllm-llama3-8b-instruct
  template:
    metadata:
      labels:
        app: vllm-llama3-8b-instruct
    spec:
      containers:
      - args:
        - --model
        - meta-llama/Llama-3.1-8B-Instruct
        - --port
        - "8000"
        - --max-loras
        - "2"
        - --lora
        - food-review-1
        image: ghcr.io/llm-d/llm-d-inference-sim:v0.1.0
        imagePullPolicy: IfNotPresent
        name: vllm-sim
        ports:
        - containerPort: 8000
          name: http
          protocol: TCP
---
apiVersion: networking.matrixinfer.ai/v1alpha1
kind: ModelRoute
metadata:
  name: llama
spec:
  modelName: "Llama-3.1"
  rules:
  - name: "default"
    targetModels:
    - modelServerName: "llama"
---
apiVersion: networking.matrixinfer.ai/v1alpha1
kind: ModelServer
metadata:
  name: llama
spec:
  workloadSelector:
    matchLabels:
      app: vllm-llama3-8b-instruct
  workloadPort:
    port: 8000
  model: "meta-llama/Llama-3.1-8B-Instruct"
  inferenceEngine: "vLLM"
  trafficPolicy:
    timeout: 10s