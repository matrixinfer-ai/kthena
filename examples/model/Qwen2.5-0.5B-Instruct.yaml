apiVersion: registry.matrixinfer.ai/v1alpha1
kind: Model
metadata:
  name: demo
spec:
  backends:
    - name: "backend1"
      type: "vLLM"
      modelURI: "hf://Qwen/Qwen2.5-0.5B-Instruct"
      cacheURI: "hostpath:///tmp/cache"
      minReplicas: 1
      maxReplicas: 1
      env:
        - name: "HF_ENDPOINT" # Optional: Use a Hugging Face mirror if you have network issues
          value: "https://hf-mirror.com/"
      workers:
        - type: "server"
          image: "public.ecr.aws/q9t5s3a7/vllm-cpu-release-repo:latest"
          replicas: 1
          pods: 1
          config:
            served-model-name: "Qwen2.5-0.5B-Instruct"
            max-model-len: 32768
            max-num-batched-tokens: 65536
            block-size: 128
            enable-prefix-caching: ""
          resources:
            limits:
              cpu: "4"
              memory: "8Gi"
            requests:
              cpu: "2"
              memory: "4Gi"
