apiVersion: registry.matrixinfer.ai/v1alpha1
kind: Model
metadata:
  annotations:
    api.kubernetes.io/name: example
  name: deepseek-r1-distill-llama-8b
spec:
  name: "deepseek-r1-distill-llama-8b"
  owner: "example"
  autoscalingPolicyRef:
    name: ""
  backends:
    - name: "deepseek-r1-distill-llama-8b-vllm"
      type: "vLLM"
      modelURI: "s3://model-bucket/deepseek-r1-distill-llama-8b"
      cacheURI: "hostpath://tmp/test"
      minReplicas: 1
      maxReplicas: 3
      scalingCost: 1
      scaleToZeroGracePeriod: 60s
      workers:
        - type: "server"
          image: "vllm/vllm-openai:v0.7.1"
          replicas: 0
          pods: 0
          config:
            maxModelLen: 12288
          resources:
            limits:
              nvidia.com/gpu: "1"
            requests:
              nvidia.com/gpu: "1"
          affinity:
      loraAdapterRefs:
        - name: ""
    - name: "deepseek-r1-distill-llama-8b-vllm-disaggregated"
      type: "vLLMDisaggregated"
      modelURI: "s3://model-bucket/deepseek-r1-distill-llama-8b"
      minReplicas: 0
      maxReplicas: 3
      scalingCost: 4
      scaleToZeroGracePeriod: 60s
      workers:
        - type: "prefill"
          image: "vllm/vllm-openai:v0.7.1"
          replicas: 2
          pods: 2
          config:
            maxModelLen: 12288
          resources:
            limits:
              nvidia.com/gpu: "1"
            requests:
              nvidia.com/gpu: "1"
          affinity:
        - type: "decode"
          image: "vllm/vllm-openai:v0.7.1"
          replicas: 2
          pods: 2
          config:
            maxModelLen: 12288
          resources:
            limits:
              nvidia.com/gpu: "1"
            requests:
              nvidia.com/gpu: "1"
          affinity:
      loraAdapterRefs:
        - name: ""
  costExpansionRatePercent: 0
