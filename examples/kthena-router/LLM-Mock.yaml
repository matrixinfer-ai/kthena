# This example shows how to deploy a mock vLLM server for testing.
# The mock server will return a fixed response for any input.
# You can use this mock server to test the inference router without deploying a real LLM server.
#
# NOTE: `ghcr.io/yaozengzeng/vllm-mock:latest` is built based on `https://github.com/YaoZengzeng/aibrix/tree/vllm-mock`.
# Move the image to kthena registry once it's public.

apiVersion: apps/v1
kind: Deployment
metadata:
  name: vllm-llama3-8b-instruct
spec:
  replicas: 3
  selector:
    matchLabels:
      app: vllm-llama3-8b-instruct
  template:
    metadata:
      labels:
        app: vllm-llama3-8b-instruct
    spec:
      containers:
        - name: llm-engine
          image: ghcr.io/yaozengzeng/vllm-mock:latest
          imagePullPolicy: IfNotPresent
          env:
            # specify the model name to mock
            - name: MODEL_NAME
              value: "meta-llama/Llama-3.1-8B-Instruct"
          command:
            - python3
            - app.py
---
apiVersion: networking.serving.volcano.sh/v1alpha1
kind: ModelRoute
metadata:
  name: llama
spec:
  modelName: "Llama-3.1"
  rules:
  - name: "default"
    targetModels:
    - modelServerName: "llama"
---
apiVersion: networking.serving.volcano.sh/v1alpha1
kind: ModelServer
metadata:
  name: llama
spec:
  workloadSelector:
    matchLabels:
      app: vllm-llama3-8b-instruct
  workloadPort:
    port: 8000
  model: "meta-llama/Llama-3.1-8B-Instruct"
  inferenceEngine: "vLLM"
  trafficPolicy:
    timeout: 10s
