apiVersion: batch/v1
kind: Job
metadata:
  name: benchmark-job
spec:
  template:
    spec:
      containers:
      - name: benchmark
        image: ghcr.io/matrixinfer-ai/benchmark:latest
        imagePullPolicy: IfNotPresent
        args:
        - --host
        - "matrixinfer-infer-gateway.matrixinfer-system"
        - --backend
        - "vllm"
        - --port
        - "80"
        - --model
        - "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B"
        - --tokenizer
        # In restricted network environment, you should pre-download the tokenizer, pack it into the docker image and use the path of downloaded tokenizer as the argument.
        # - "/root/.cache/huggingface/hub/models--deepseek-ai--DeepSeek-R1-Distill-Qwen-7B/snapshots/916b56a44061fd5cd7d6a8fb632557ed4f724f60/
        - "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B"
        - --dataset-name
        - "generated-shared-prefix"
        - --gsp-num-groups
        - "256"
        - --gsp-prompts-per-group
        - "16"
        - --gsp-system-prompt-len
        - "256"
        - --gsp-question-len
        - "2048"
        - --gsp-output-len
        - "256"
        - --request-rate
        - "800"
        - --max-concurrency
        - "300"
        # If you don't want to generate new test data every time, you can mount the volume to the cache directory.
        #volumeMounts:
        #  - name: benchmark-cache
        #    mountPath: /root/.cache/sglang/benchmark
      restartPolicy: Never
  backoffLimit: 0  # Don't retry on failure 