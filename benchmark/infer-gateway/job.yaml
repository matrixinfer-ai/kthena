apiVersion: batch/v1
kind: Job
metadata:
  name: benchmark-job
spec:
  template:
    # Unncessary if you don't want to cache generated test data in hostpath.
    nodeSelector:
      kubernetes.io/hostname: 192.168.0.126
    spec:
      containers:
      - name: benchmark
        image: ghcr.io/matrixinfer-ai/benchmark:latest
        imagePullPolicy: IfNotPresent
        args:
        - --host
        - "matrixinfer-infer-gateway.matrixinfer-system"
        - --backend
        - "vllm"
        - --port
        - "80"
        - --model
        - "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B"
        - --tokenizer
        # In restricted network environment, you should pre-download the tokenizer, pack it into the docker image and use the path of downloaded tokenizer as the argument.
        # - "/root/.cache/huggingface/hub/models--deepseek-ai--DeepSeek-R1-Distill-Qwen-7B/snapshots/916b56a44061fd5cd7d6a8fb632557ed4f724f60/
        - "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B"
        - --dataset-name
        - "generated-shared-prefix"
        - --gsp-num-groups
        - "256"
        - --gsp-prompts-per-group
        - "16"
        - --gsp-system-prompt-len
        - "256"
        - --gsp-question-len
        - "2048"
        - --gsp-output-len
        - "256"
        - --request-rate
        - "800"
        - --max-concurrency
        - "300"
        volumeMounts:
        - name: benchmark-cache
          mountPath: /root/.cache/sglang/benchmark
      restartPolicy: Never
      volumes:
      # Change to any type of kubernetes storage type you want.
      # If you want to generate new test data every time, then you don't need to mount any volume.
      - name: benchmark-cache
        hostPath:
          path: /root/sglang-cache
          type: Directory
  backoffLimit: 0  # Don't retry on failure 