apiVersion: registry.matrixinfer.ai/v1
kind: Model
metadata:
  labels:
    app.kubernetes.io/name: matrixinfer
    app.kubernetes.io/managed-by: kustomize
  name: deepseek-r1-distill-llama-8b
spec:
  name: "deepseek-r1-distill-llama-8b"
  owner: "example"
  autoscalingPolicyRef:
    name: ""
  backends:
    - name: "deepseek-r1-distill-llama-8b-vllM"
      type: "vLLM"
      config:
        maxModelLen: 12288
      modelURI: "s3://model-bucket/deepseek-r1-distill-llama-8b"
      cacheURI: "hostPath://tmp/test"
      minReplicas: 1
      maxReplicas: 3
      cost: 1
      scaleToZeroGracePeriod: 60s
      workers:
        - type: "server"
          image: "vllm/vllm-openai:v0.7.1"
          replicas: 0
          pods: 0
          resources:
            limits:
              nvidia.com/gpu: "1"
            requests:
              nvidia.com/gpu: "1"
          affinity:
      loraAdapterRefs:
        - name: ""
      autoscalingPolicyRef:
        name: ""
    - name: "deepseek-r1-distill-llama-8b-vllM-disaggregated"
      type: "vLLMDisaggregated"
      config:
        maxModelLen: 12288
      modelURI: "s3://model-bucket/deepseek-r1-distill-llama-8b"
      minReplicas: 0
      maxReplicas: 3
      cost: 4
      scaleToZeroGracePeriod: 60s
      workers:
        - type: "prefill"
          image: "vllm/vllm-openai:v0.7.1"
          replicas: 2
          pods: 2
          resources:
            limits:
              nvidia.com/gpu: "1"
            requests:
              nvidia.com/gpu: "1"
          affinity:
        - type: "decode"
          image: "vllm/vllm-openai:v0.7.1"
          replicas: 2
          pods: 2
          resources:
            limits:
              nvidia.com/gpu: "1"
            requests:
              nvidia.com/gpu: "1"
          affinity:
      loraAdapterRefs:
        - name: ""
      autoscalingPolicyRef:
        name: ""
  costExpansionRatePercent: 0

